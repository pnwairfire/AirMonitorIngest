---
title: "AQS 88101 'Known Locations'"
author: "Jonathan Callahan"
date: "September 27, 2021"
output:
  html_document:
    toc: TRUE
    toc_depth: 3
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The USFS [AirFire](https://www.airfire.org) group is focused on air quality 
associated with wildfire smoke and maintains both historical and 
real-time databases of air quality monitoring
data obtained from stationary monitors (_i.e._ not _"mobile monitors"_). This data
is used in operational displays and for retrospective analysis. Data ingest
and management of air quality "stationary time series" is thus an important ongoing activity.

Challenges include:
 * Location stability -- jitter in reported longitude and latitude coming from GPS.
 * Data duplication -- data from an individual monitor arriving through separate data streams.
 * CPU-intense spatial searches for data such as time zone.
 
To address these and other issues, we are creating a "known locations" table -- 
a set of which locations which have been vetted for reuse by AirFire and for which
all CPU-intensive spatial queries have already been performed.

Isolating purely spatial information from monitor-related information will allow
us to quickly assign incoming data to a "known location" and immediately retrieve
associated spatial data that would otherwise require expensive spatial calculations.

The basic concept for creating a _"known locations"_ table is described in the introduction to the 
[MazamaLocationUtils](https://mazamascience.github.io/MazamaLocationUtils/)
R package.

Because the focus is on wildifres, the spatial scale of interest is >= 500 meters and
each "known location" will be understood to represent a circle with a radius of 
250 meters. Any incoming data records whose longitude and latitude fall within 
250m of an existing location wll assigned to that "known location". New "known locations"
will be added as needed.

# Setup

```{r mazama_setup, message = FALSE}
library(MazamaCoreUtils)

library(AirMonitorIngest)

# Create a directory specifically for EPA data
dir.create("~/Data/EPA", showWarnings = FALSE, recursive = TRUE)
```

We will begin with an existing database of EPA AQS site information. This 
database is assumed to be largely well vetted and contain most of the relevant spatial 
information we will need for further data processing and visualization.

## Download Data

### AQS sites

The `AQS_sites` table contains primarily spatial location information with additional
_"site"_ information that is specific to an individual monitor, _e.g._
`Owning Agency`. As we will see, it is possible to have multiple _"sites"_ at a 
single _"known location"_.

```{r AQS_sites, message = FALSE, results = "hold"}
# Get site metadata
AQS_sites <- epa_getAQSSites(downloadDir = "~/Data/EPA")

dim(AQS_sites)
names(AQS_sites)
```

### AQS Monitors

The `AQS_monitors` table mixes monitor-specific information like `Parameter Code`
and `Last Sample Data` with location-specific information like `State Code` or
`CBSA Name`.

```{r AQS_monitors, message = FALSE, results = "hold"}
# Get monitor metadata
AQS_monitors <- epa_getAQSMonitors(downloadDir = "~/Data/EPA")

dim(AQS_monitors)
names(AQS_monitors)
```

### AQS Parameter Codes

The `AQS_parameterCodes` table provides detailed information on each individual
parameter (pollutant) found in the AQS data.

```{r AQS_parameterCodes, message = FALSE, results = "hold"}
# Get parameter codes
AQS_parameterCodes <- epa_getAQSCodes(tableName = "parameters")

dim(AQS_parameterCodes)
names(AQS_parameterCodes)
```

----

## Explore `AQS_monitors`

### Parameter Codes

The two most important parameter codes for AirFire PM2.5 measurements are
`"88101"` and `"88502"`. 

#### 88101

```{r parameter_88101, results = "hold"}
AQS_parameterCodes %>%
  dplyr::filter(`Parameter Code` %in% c("88101")) %>% 
  dplyr::glimpse()
```

#### 88502

```{r parameter_88502, results = "hold"}
AQS_parameterCodes %>%
  dplyr::filter(`Parameter Code` %in% c("88502")) %>% 
  dplyr::glimpse()
```

### Measurement Scale

As seen in the output below, `Measurement Scale` provides important 
information on the appropriate use of data from a particular monitor. The 
AirFire _"known locations"_ table will be used in the context of wildfire smoke
over large regions. For this use case, we will ignore measurement scales < 500m.

```{r measurement_scale, results = "hold"}
AQS_monitors %>%
  dplyr::mutate(scale_definition = paste(`Measurement Scale`, `Measurement Scale Definition`, sep = ' -- ')) %>%
  dplyr::pull(`scale_definition`) %>%
  unique() %>% sort()
```

----

## Subset for Sites with Parameter Code 88101

### Subset Monitors

We begin by subsetting `AQS_monitors` to include only those monitors used in
regulatory decision making -- PM2.5 FRM/FEM or `88101` and only those associated
with `POC == ` -- the first monitor deployed at an AQS _site_. We will use state,
county and site information to create an `AQSID` which is typically associated
with a monitor-site. The `AQSID can be created for both `AQS_monitors` and
`AQS_sites`, allowing us to create queries combining the two.

```{r AQS_monitors_0, results = "hold"}
AQS_monitors_0 <-
  AQS_monitors %>%
  # Subset
  dplyr::filter(`Measurement Scale` != "MICROSCALE") %>%
  dplyr::filter(`Measurement Scale` != "MIDDLE SCALE") %>%
  dplyr::filter(`Parameter Code` == "88101") %>%
  dplyr::filter(`POC` == "1") %>%
  # Add AQSID as a unique identifier
  dplyr::mutate(
    AQSID = paste0(`State Code`, `County Code`, `Site Number`)
  )

dim(AQS_monitors_0)
```

### Subset Sites

```{r AQS_sites_0, results = "hold"}
AQS_sites_0 <-
  AQS_sites %>%
  # Add AQSID as a unique identifier
  dplyr::mutate(
    AQSID = paste0(`State Code`, `County Code`, `Site Number`)
  ) %>%
  # Subset
  dplyr::filter(AQSID %in% AQS_monitors_0$AQSID)

dim(AQS_sites_0)
```

----

## Create a "Known Locations" Table

We will build a standardized _"known locations"_ table using the 
*MazamaLocationUtils*. This will involve:

* creating standardized variables
* adding spatial metadata

We begin by initializing *MazamaLocationUtils* and associated datasets and
then display the column names we need to create.

```{r known_location_columns, results = "hold"}
library(MazamaSpatialUtils)
library(MazamaLocationUtils)

# Set up spatial data
MazamaLocationUtils::mazama_initialize("~/Data/Spatial")

# Print out names for an empty "known locations" table
known_location_columns <-
  MazamaLocationUtils::table_initialize() %>% 
  names()
```

### Create Standard Variables

The recipe below performs this in stages primarily using functions from *dplyr*.

```{r harmonizing_variables, results = "hold"}
known_locations_0 <-
  
  # Start with AQS_sites_0
  AQS_sites_0 %>%

  # Rename all existing columns with "AQS_"
  dplyr::rename_all(make.names) %>%
  dplyr::rename_all(~ gsub("^", "AQS_", .x)) %>%

  # Add "known location" columns derived from AQS columns where possible
  dplyr::mutate(
    locationID = as.character(NA),
    locationName = AQS_Local.Site.Name,
    longitude = AQS_Longitude,
    latitude = AQS_Latitude,
    elevation = AQS_Elevation,
    countryCode = as.character(NA),
    stateCode = MazamaSpatialUtils::US_stateFIPSToCode(AQS_State.Code),
    county = AQS_County.Name,
    timezone = as.character(NA),
    houseNumber = as.character(NA),
    street = as.character(NA),
    city = AQS_City.Name,
    zip = AQS_Zip.Code
  )
```

For convenience of use with `View()`, we will reorganize the columns with "known location" columns
first and get a sense of our data by creating a map of all locations:

```{r reorganize_columns, results = "hold"}
# Get "AQS_" columns
AQS_columns <-
  names(known_locations_0) %>%
  stringr::str_subset("AQS_.*")

# Reorder column names
known_locations_0 <-
  known_locations_0 %>%
  dplyr::select(dplyr::all_of(c(known_location_columns, AQS_columns)))

plot(
  known_locations_0$longitude, known_locations_0$latitude, 
  pch = 15, cex = 0.5, col  = "red",
  xlab = "", ylab = "",
  main = "AQS 88101 Known Locations"
)

maps::map("world", add = TRUE)
```

### Add/fix Required Columns

Several columns of data were set to `NA` when we created `known_locations_0`.
These columns require a bit more work and are dealt with here:

```{r fix_columns, results = "markdown"}
# Add ISO countryCodes
known_locations_0$countryCode <-
  dplyr::case_when(
    known_locations_0$AQS_State.Code == "66" ~ "GU",
    known_locations_0$AQS_State.Code == "72" ~ "PR",
    known_locations_0$AQS_State.Code == "78" ~ "VI",
    known_locations_0$AQS_State.Code == "80" ~ "MX",
    known_locations_0$AQS_State.Code == "CC" ~ "CA",
    TRUE ~ "US" # default
  )

# Add unique identifiers
known_locations_0$locationID <-
  MazamaLocationUtils::location_createID(
    known_locations_0$longitude,
    known_locations_0$latitude
  )

# Add timezones
known_locations_0$timezone <-
  MazamaSpatialUtils::getTimezone(
    known_locations_0$longitude,
    known_locations_0$latitude,
    # NOTE:  EPA has monitors from US, Canada, Mexico, Puerto Rico, Virgin Islands and Guam
    countryCodes = c("US", "CA", "MX", "PR", "VI", "GU"),
    useBuffering = TRUE
)

# TODO:  Fix stateCode where countryCode != "US"
```

### Review

It's now time to review our first pass table, `known_locations_0`:

```{r first_pass_review}
# What country codes to we have?
table(known_locations_0$countryCode)

# Are all locations unique?
any(duplicated(known_locations_0$locationID))
```
### Impose 500 Meter Separation

As described in the Introduction, we want the AirFire "known locations" table to
have a measurement scale >= 500 meters. We will use functions from
*MazamaLocationUtils* to identify overlapping locations and deal with them on an
individual basis.

### Find Overlapping Locations

```{r find_overlapping}
distance_table <- 
  dplyr::select(known_locations_0, longitude, latitude) %>%
  MazamaLocationUtils::table_findOverlappingDistances(radius = 250)

print(distance_table)
```

So we have `r nrow(distance_table)*2` separate _"sites"_ involved in `r nrow(distance_table)` overlaps 
given our chosen 500m separation rule. Further investigation is in order.

### Leaflet Inspection

We can use the `leaflet_table()` function to examine overlapping _"sites"_ and
make executive decisions based on other metadata as to which one we will use for our _"known location"_
representing that area.

```{r leaflet}
source("table_leaflet.R")
# Which sites are too close
known_locations_0 %>%
  MazamaLocationUtils::table_findOverlappingLocations(radius = 250) %>%
  table_leaflet()
```

### Manual Editing

We will work our way through all "too close" sites and 
decide which locations should be elided from our location table. We hide the
details but show the results:

```{r manual_inspection, echo = FALSE}
IDs_for_removal <- c()

# ----- Alaska  -----

# AQSID 020900040 or 020900029 ?
# ==> 020900029 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "020900029")

# ----- Washington -----

# AQSID 530330037 or 530330004 ?
# ==> 530330004 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "530330004")

# ----- Idaho -----

# AQSID 160170005 or 160170004 ?
# ==> xxx because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "160170004")

# AQSID 160270004 or 160270005 ?
# ==> 160270005 because location and site name do not match
IDs_for_removal <- append(IDs_for_removal, "160270005")

# ----- Oregon -----

# AQSID 410250003 or 410250002 ?
# ==> 410250002 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "410250002")

# AQSID 410390059 or 410390058 ?
# ==> 410390058 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "410390058")

# AQSID 410330107 or 410330114 ?
# ==> 410330114 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "410330114")

# ----- California -----

# AQSID 060190011 or 060190008 ?
# ==> 060190008 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "060190008")

# AQSID 060731022 or 060730003 ?
# ==> 060730003 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "060730003")

# AQSID 060250007 or 060250003 ?
# ==> 060250003 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "060250003")

# ----- South Dakota -----

# AQSID 461030020 or 461030019 ?
# ==> 461030019 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "461030019")

# ----- Nebraska -----

# AQSID 311570006 or 311570004 ?
# ==> 311570004 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "311570004")

# ----- Colorado -----

# AQSID 080010006 or 080010001 ?
# ==> 080010001 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "080010001")

# AQSID 080070001 or 080070002 ?
# ==> 080070002 because it stopped collecting data in 2001
IDs_for_removal <- append(IDs_for_removal, "080070002")

# ----- New Mexico -----

# AQSID 350439011 or 350439003 ?
# ==> 350439003 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "350439003")

# AQSID 350019013 or 350019004 ?
# ==> 350019004 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "350019004")

# ----- Texas -----

# AQSID 484393010 or 484390063 ?
# ==> 484390063 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "484390063")

# ----- Florida -----

# AQSID 120992005 or 120992003 ?
# ==> 120992003 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "120992003")

# ----- Puerto Rico -----

# AQSID 720570012 or 720570008 ?
# ==> 720570008 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "720570008")

# ----- South Carolina -----

# AQSID 450450014 or 450450008 ?
# ==> 450450008 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "450450008")

# ----- Missouri -----

# AQSID 290950041 or 290470041 ?
# ==> 290470041 because it location and county do not match
IDs_for_removal <- append(IDs_for_removal, "290470041")

# AQSID 291892003 or 291893001 ?
# ==> 291893001 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "291893001")

# ----- Illinois -----

# AQSID 171132003 or 171132002 ?
# ==> 171132002 because it stopped collecting data in 2004
IDs_for_removal <- append(IDs_for_removal, "171132002")

# AQSID 171613002 or 171610003 ?
# ==> 171610003 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "171610003")

# ----- Wisconsin -----

# AQSID 550790056 or 550790058 ?
# ==> 550790058 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "550790058")

# ----- Ohio -----

# AQSID 390950028 or 390950025 ?
# ==> 390950025 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "390950025")

# AQSID 391130032 or 391130014 ?
# ==> 391130014 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "391130014")

# AQSID 390490034 or 390490024 ?
# ==> 390490024 because it stopped collecting data in 2018
IDs_for_removal <- append(IDs_for_removal, "390490024")

# AQSID 390490039 or 390490025 ?
# ==> 390490025 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "390490025")

# AQSID 391550014 or 391550005 ?
# ==> 391550005 because it stopped collecting data in 2015
IDs_for_removal <- append(IDs_for_removal, "391550005")

# AQSID 390810017 or 390810016 ?
# ==> 390810016 because it stopped collecting data in 2003
IDs_for_removal <- append(IDs_for_removal, "390810016")

# AQSID 390810021 or 390811001 ?
# ==> 390811001 because it stopped collecting data in 2013
IDs_for_removal <- append(IDs_for_removal, "390811001")

# ----- West Virginia -----

# AQSID 540390010 or 540390009 ?
# ==> 540390009 because it stopped collecting data in 2000
IDs_for_removal <- append(IDs_for_removal, "540390009")

# ----- Washington DC -----

# AQSID 240330025 or 240330001 ?
# ==> 240330001 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "240330001")

# ----- New Jersey -----

# AQSID 340030008 or 340030007 ?
# ==> 340030007 because the AQSID implies an earlier deployment
IDs_for_removal <- append(IDs_for_removal, "340030007")

# AQSID 340170008 or 340172002 ?
# ==> 340172002 because it was deployed earlier and has less metadata
IDs_for_removal <- append(IDs_for_removal, "340172002")

# ----- New York -----

# AQSID 360810125 or 360810124 ?
# ==> 360810124 because it was deployed earlier
IDs_for_removal <- append(IDs_for_removal, "360810124")
```

```{r show_IDs_for_removal}
print(IDs_for_removal)
```

## Final "Known Locations" Table

### Subset

Removing these monitors will leave us with a table where every monitor is at least 500 meters away from their nearest neighbor.

```{r final_known_locations}
dim(known_locations_0)

known_locations <-
  known_locations_0 %>%
  dplyr::filter(!AQS_AQSID %in% IDs_for_removal)

dim(known_locations)

```

### Consistent Casing

```{r consistent casing}
known_locations$locationName <- 
  stringr::str_to_title(known_locations$locationName)
```

### Non-US State Codes

Using ISO standards requires that we re-evaluate states codes for non-US countries.

```{r ISO_stateCode}

# Split, fix and then recombine
US <- dplyr::filter(known_locations, countryCode == "US")
non_US <- dplyr::filter(known_locations, countryCode != "US")

non_US$stateCode <-
  MazamaSpatialUtils::getStateCode(
    non_US$longitude,
    non_US$latitude,
    dataset = "NaturalEarthAdm1",
    countryCodes = c("CA", "MX", "VI", "GU"),
    useBuffering = TRUE
  )

# Combine two tables
known_locations <-
  dplyr::bind_rows(
    US,
    non_US
  )
```

### Minor improvements

```{r minor improvements}
known_locations <-
  known_locations %>%
  dplyr::mutate(city = dplyr::na_if(city, "Not in a City")) %>%
  dplyr::mutate(city = dplyr::na_if(city, "Not in a city"))
```

# Final Result

```{r final_result}
table_leaflet(known_locations)
```

## Compare with removed locations

```{r compare_with_removed}
removed_locations <-
  known_locations_0 %>%
  dplyr::filter(AQS_AQSID %in% IDs_for_removal)

map <- table_leaflet(known_locations)

table_leafletAdd(map, removed_locations)
```

```{r save}
setLocationDataDir("~/Data/Known_Locations")
table_save(known_locations, "AirFire_monitors_500m")

table_csv <- table_export(known_locations)
readr::write_csv(known_locations, file = file.path(getLocationDataDir(), "AirFire_monitors_500m.csv"))
```

