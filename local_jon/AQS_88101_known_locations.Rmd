---
title: "EPA AQS 88101 'Known Locations'"
author: "Jonathan Callahan"
date: "September 29, 2021"
output:
  html_document:
    toc: TRUE
    toc_depth: 3
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The USFS [AirFire](https://www.airfire.org) group is focused on air quality 
measurements associated with wildfire smoke and maintains both historical and 
real-time databases of PM2.5 monitoring data obtained from stationary monitors 
(_i.e._ not _"mobile monitors"_). This data is used in operational displays and 
for retrospective analysis. Data ingest and management of air quality 
"stationary time series" are both important ongoing activities.

Challenges include:
 * Location stability -- jitter in reported longitude and latitude coming from 
 real-time GPS.
 * Data duplication -- data from an individual monitor arriving through mutiple 
 data streams.
 * CPU-intensive spatial searches for data such as time zone.
 
To address these and other issues, we are creating "known locations" tables -- 
a set of which locations which have been vetted for reuse and for which all 
CPU-intensive spatial queries have already been performed.

Separating purely spatial information from other monitor-related metadata will 
allow us to quickly assign incoming data to a "known location" and immediately 
retrieve associated spatial data that would otherwise require expensive spatial 
calculations.

The basic concept for creating a _"known locations"_ table is described in the 
introduction to the 
[MazamaLocationUtils](https://mazamascience.github.io/MazamaLocationUtils/)
R package.

For our initial "known locations" table, we will focus on AQS parameter 88101
(PM2.5 FRM/FEM) and measurement scales at the "NEIGHBORHOOD" scale and higher 
(>= 100 m), ignoring any "MICROSCALE" measurements found in the EPA AQS database. 
An initial set of "known locations" will be created from EPA AQS "sites" 
metadata, augmented with additional suse patial data.

# Setup

```{r mazama_setup, message = FALSE}
library(MazamaCoreUtils)

library(AirMonitorIngest)

# Create a directory specifically for EPA data
dir.create("~/Data/EPA", showWarnings = FALSE, recursive = TRUE)
```

We will begin with an existing database of EPA AQS site information. This 
database is assumed to be largely well vetted and contain most of the relevant 
spatial information we will need for further data processing and visualization.

## Download Data

### AQS sites

The `AQS_sites` table contains primarily spatial location information with additional
_"site"_ information that is specific to an individual monitor, _e.g._
`Owning Agency`. As we will see, it is possible to have multiple _"sites"_ at a 
single _"known location"_.

```{r AQS_sites, message = FALSE, results = "hold"}
# Get site metadata
AQS_sites <- epa_aqs_getSites(downloadDir = "~/Data/EPA")

dim(AQS_sites)
names(AQS_sites)
```

### AQS Monitors

The `AQS_monitors` table mixes monitor-specific information like `Parameter Code`
and `Last Sample Data` with location-specific information like `State Code` or
`CBSA Name`.

```{r AQS_monitors, message = FALSE, results = "hold"}
# Get monitor metadata
AQS_monitors <- epa_aqs_getMonitors(downloadDir = "~/Data/EPA")

dim(AQS_monitors)
names(AQS_monitors)
```

### AQS Parameter Codes

The `AQS_parameterCodes` table provides detailed information on each individual
parameter (pollutant) found in the AQS data.

```{r AQS_parameterCodes, message = FALSE, results = "hold"}
# Get parameter codes
AQS_parameterCodes <- epa_aqs_getCodes(tableName = "parameters")

dim(AQS_parameterCodes)
names(AQS_parameterCodes)
```

----

## Explore `AQS_monitors`

### Parameter Codes

Parameter code 88101 is associated with regulatory PM2.5 measurements: 

#### 88101

```{r parameter_88101, results = "hold"}
AQS_parameterCodes %>%
  dplyr::filter(`Parameter Code` %in% c("88101")) %>% 
  dplyr::glimpse()
```

### Measurement Scale

As seen in the output below, `Measurement Scale` provides important 
information on the appropriate use of data from a particular monitor. The 
AirFire _"known locations"_ table will be used in the context of wildfire smoke
over large regions. For this use case, we will ignore measurement scales < 500m.

```{r measurement_scale, results = "hold"}
AQS_monitors %>%
  dplyr::mutate(scale_definition = paste(`Measurement Scale`, `Measurement Scale Definition`, sep = ' -- ')) %>%
  dplyr::pull(`scale_definition`) %>%
  unique() %>% sort()
```

----

## Subset for Sites with Parameter Code 88101

### Subset Monitors

We begin by subsetting `AQS_monitors` to include only those monitors used in
regulatory decision making -- PM2.5 FRM/FEM or `88101`. We will use state,
county and site information to create an `AQSID` which is typically associated
with a monitor-site. The `AQSID can be created for both `AQS_monitors` and
`AQS_sites`, allowing us to create queries combining the two.

```{r AQS_monitors_0, results = "hold"}
AQS_monitors_0 <-
  AQS_monitors %>%
  # Subset
  dplyr::filter(`Measurement Scale` != "MICROSCALE") %>%
  dplyr::filter(`Parameter Code` == "88101") %>%
  # Add AQSID as a unique identifier
  dplyr::mutate(
    AQSID = paste0(`State Code`, `County Code`, `Site Number`)
  )

dim(AQS_monitors_0)
```

### Subset Sites

```{r AQS_sites_0, results = "hold"}
AQS_sites_0 <-
  AQS_sites %>%
  # Add AQSID as a unique identifier
  dplyr::mutate(
    AQSID = paste0(`State Code`, `County Code`, `Site Number`)
  ) %>%
  # Subset
  dplyr::filter(AQSID %in% AQS_monitors_0$AQSID)

dim(AQS_sites_0)
```

----

## Create a "Known Locations" Table

We will build a standardized _"known locations"_ table using the 
*MazamaLocationUtils*. This will involve:

* creating standardized variables
* adding spatial metadata

We begin by initializing *MazamaLocationUtils* and associated datasets and
then display the column names we need to create.

```{r known_location_columns, results = "hold"}
library(MazamaSpatialUtils)
library(MazamaLocationUtils)

# Set up spatial data
MazamaLocationUtils::mazama_initialize("~/Data/Spatial")

# Print out names for an empty "known locations" table
known_location_columns <-
  MazamaLocationUtils::table_initialize() %>% 
  names()
```

### Create Standard Variables

The recipe below performs this in stages primarily using functions from *dplyr*.

```{r harmonizing_variables, results = "hold"}
known_locations <-
  
  # Start with AQS_sites_0
  AQS_sites_0 %>%

  # Rename all existing columns with "AQS_"
  dplyr::rename_all(make.names) %>%
  dplyr::rename_all(~ gsub("^", "AQS_", .x)) %>%

  # Add "known location" columns derived from AQS columns where possible
  dplyr::mutate(
    locationID = as.character(NA),
    locationName = AQS_Local.Site.Name,
    longitude = AQS_Longitude,
    latitude = AQS_Latitude,
    elevation = AQS_Elevation,
    countryCode = as.character(NA),
    stateCode = MazamaSpatialUtils::US_stateFIPSToCode(AQS_State.Code),
    county = AQS_County.Name,
    timezone = as.character(NA),
    houseNumber = as.character(NA),
    street = as.character(NA),
    city = AQS_City.Name,
    zip = AQS_Zip.Code
  )
```

For convenience of use with `View()`, we will reorganize the columns with "known location" columns
first and get a sense of our data by creating a map of all locations:

```{r reorganize_columns, results = "hold"}
# Get "AQS_" columns
AQS_columns <-
  names(known_locations) %>%
  stringr::str_subset("AQS_.*")

# Reorder column names
known_locations <-
  known_locations %>%
  dplyr::select(dplyr::all_of(c(known_location_columns, AQS_columns)))

plot(
  known_locations$longitude, known_locations$latitude, 
  pch = 15, cex = 0.5, col  = "red",
  xlab = "", ylab = "",
  main = "AQS 88101 Known Locations"
)

maps::map("world", add = TRUE)
```

### Add/fix Required Columns

Several columns of data were set to `NA` when we created `known_locations`.
These columns require a bit more work and are dealt with here:

```{r fix_columns, results = "markdown"}
# Add ISO countryCodes
known_locations$countryCode <-
  dplyr::case_when(
    known_locations$AQS_State.Code == "66" ~ "GU",
    known_locations$AQS_State.Code == "72" ~ "PR",
    known_locations$AQS_State.Code == "78" ~ "VI",
    known_locations$AQS_State.Code == "80" ~ "MX",
    known_locations$AQS_State.Code == "CC" ~ "CA",
    TRUE ~ "US" # default
  )

# Add unique identifiers
known_locations$locationID <-
  MazamaLocationUtils::location_createID(
    known_locations$longitude,
    known_locations$latitude
  )

# Add timezones
known_locations$timezone <-
  MazamaSpatialUtils::getTimezone(
    known_locations$longitude,
    known_locations$latitude,
    # NOTE:  EPA has monitors from US, Canada, Mexico, Puerto Rico, Virgin Islands and Guam
    countryCodes = c("US", "CA", "MX", "PR", "VI", "GU"),
    useBuffering = TRUE
)
```

### Review

It's now time to review our first pass table, `known_locations`:

```{r first_pass_review}
# What country codes to we have?
table(known_locations$countryCode)

# Are all locations unique?
any(duplicated(known_locations$locationID))
```

## Final "Known Locations" Table

### Consistent Casing

```{r consistent casing}
known_locations$locationName <- 
  stringr::str_to_title(known_locations$locationName)
```

### Non-US State Codes

Using ISO standards requires that we re-evaluate state codes for non-US countries.

```{r ISO_stateCode}

# Split, fix and then recombine
US <- dplyr::filter(known_locations, countryCode == "US")
non_US <- dplyr::filter(known_locations, countryCode != "US")

non_US$stateCode <-
  MazamaSpatialUtils::getStateCode(
    non_US$longitude,
    non_US$latitude,
    dataset = "NaturalEarthAdm1",
    countryCodes = c("CA", "MX", "VI", "GU"),
    useBuffering = TRUE
  )

# Combine two tables
known_locations <-
  dplyr::bind_rows(
    US,
    non_US
  )
```

### Minor improvements

```{r minor improvements}
known_locations <-
  known_locations %>%
  dplyr::mutate(city = dplyr::na_if(city, "Not in a City")) %>%
  dplyr::mutate(city = dplyr::na_if(city, "Not in a city"))
```

### Remove AQS_ parameters

```{r review_AQS_, results = "hold"}
# Review "AQS_" parameters for removal
lapply(known_locations, function(x) { sum(is.na(x)) }) %>% str()
```

```{r remove_non_spatial, results = "hold"}
# Remove unwanted columns
unwanted_columns <- c(
  # Renamed or redundant
  "AQS_Latitude",
  "AQS_Longitude",
  "AQS_Elevation",
  "AQS_Zip.Code",
  "AQS_State.Name",
  "AQS_County.Name",
  "AQS_City.Name",
  # Not useful or potentially no longer accurate
  "AQS_Datum",
  "AQS_Land.Use",
  "AQS_Location.Setting",
  # Very little data found
  "AQS_Met.Site.State.Code",
  "AQS_Met.Site.County.Code",
  "AQS_Met.Site.Site.Number",
  "AQS_Met.Site.Type",
  "AQS_Met.Site.Distance",
  "AQS_Met.Site.Direction"
)

known_locations <- 
  known_locations %>%
  dplyr::select(- all_of(unwanted_columns))


```


# Final Result

```{r final_result}
table_leaflet(
  known_locations,
  extraVars = c("AQS_Site.Established.Date", "AQS_Site.Closed.Date")
)
```


```{r save}
setLocationDataDir("~/Data/Known_Locations")
table_save(known_locations, "AQS_88101")

table_csv <- table_export(known_locations)
readr::write_csv(known_locations, file = file.path(getLocationDataDir(), "AQS_88101.csv"))
```

