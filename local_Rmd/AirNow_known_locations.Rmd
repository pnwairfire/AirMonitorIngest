---
title: "AirNow: Sites 'known locations' Table"
author: "Jonathan Callahan"
date: "October 27, 2021"
output:
  html_document:
    toc: TRUE
    toc_depth: 3
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This tutorial demonstrates how to use the **MazamaLocationUtils** and
**AirMonitorIngest** R packages to work with AirNow sites data. The result will 
be a "known locations" table for AirNow "permanent" monitors.

# Setup

We begin by loading sites metadata from AirNow:

```{r setup_data, message = FALSE}
library(MazamaCoreUtils)
library(AirMonitorIngest)

airnow_sites <- airnow_getSites()
```

----

# Standardize the Sites Table

We will modify the `airnow` table using the  **MazamaLocationUtils** package. 
This will involve:

* creating standardized variables
* adding spatial metadata

We begin by initializing **MazamaLocationUtils** and associated datasets and
then display the column names we need to create.

```{r known_location_columns, results = "hold"}
library(MazamaSpatialUtils)
library(MazamaLocationUtils)

# Set up spatial data
MazamaLocationUtils::mazama_initialize("~/Data/Spatial")

# Print out names for an empty "known locations" table
known_location_columns <-
  MazamaLocationUtils::table_initialize() %>% 
  names()
```

Standardization of `airnow_sites` will take place in several 
stages.

## Create required variables

The **MazamaLocationUtils** package functionality relies on the presence of a 
set of standardized columns being present in a `locationTbl`.

* locationID
* locationName
* longitude
* latitude
* elevation
* countryCode
* stateCode
* countyName
* timezone
* houseNumber
* street
* city
* zip

We begin by renaming existing columns of data and then adding all required 
columns. Some are filled in here. Others will be handled below.

```{r harmonizing_variables, results = "hold"}
sites_locationTbl <-
  
  # Start with airnow_sites
  airnow_sites %>%
  
  # Drop the "empty" fields
  dplyr::select(-dplyr::starts_with("empty")) %>%
  
  # Filter for PM2.5 in North America
  dplyr::filter(parameterName == "PM2.5") %>%
  dplyr::filter(GMTOffsetHours < 0) %>%
  dplyr::filter(latitude > 15.0) %>%

  # Rename all existing columns with "airnow_"
  dplyr::rename_all(~ gsub("^", "airnow_", .x)) %>%

  # Add "known location" columns derived from AirNow columns where possible
  dplyr::mutate(
    locationID = as.character(NA),
    # locationName = airnow_siteName,
    # longitude = airnow_longitude,
    # latitude = airnow_latitude,
    # elevation = airnow_elevation,
    # countryCode = airnow_countryCode,
    # stateCode = airnow_stateCode,
    # countyName = airnow_countyName,
    timezone = as.character(NA),
    houseNumber = as.character(NA),
    street = as.character(NA),
    city = as.character(NA),
    zip = as.character(NA)
  ) %>%
  
  # Rename columns where the data esists
  dplyr::rename(
    locationName = airnow_siteName,
    longitude = airnow_longitude,
    latitude = airnow_latitude,
    elevation = airnow_elevation,
    countryCode = airnow_countryCode,
    stateCode = airnow_stateCode,
    countyName = airnow_countyName
  ) %>%
  
  # Remove records with missing lons or lats
  dplyr::filter(
    is.finite(.data$longitude),
    is.finite(.data$latitude),
    .data$longitude != 0,
    .data$latitude != 0
  )
```

## Reorganize columns

We reorder the columns at this point with "known location" columns first. This
will make it easier to review the data with functions like `View()`.

```{r reorganize_columns, results = "hold"}
# Get "airnow_" columns
airnow_columns <-
  names(sites_locationTbl) %>%
  stringr::str_subset("airnow_.*")

# Reorder column names
sites_locationTbl <-
  sites_locationTbl %>%
  dplyr::select(dplyr::all_of(c(known_location_columns, airnow_columns)))
```

## Add/fix required columns

Several columns of data were set to `NA` when we created `sites_locationTbl`.
These columns require a bit more work and are dealt with here:

```{r fix_columns, results = "markdown"}
# ----- Add unique identifiers -------------------------------------------------

sites_locationTbl$locationID <-
  MazamaLocationUtils::location_createID(
    sites_locationTbl$longitude,
    sites_locationTbl$latitude
  )

# ----- Add timezones ----------------------------------------------------------

sites_locationTbl$timezone <-
  MazamaSpatialUtils::getTimezone(
    sites_locationTbl$longitude,
    sites_locationTbl$latitude,
    # NOTE:  EPA has monitors from US, Canada, Mexico, Puerto Rico, Virgin Islands and Guam
    countryCodes = c("US", "CA", "MX", "PR", "VI", "GU"),
    useBuffering = TRUE
)

# ----- Replace countryCodes ---------------------------------------------------

# NOTE:  Puerto Rico seems to be the only mis-assigned country code

sites_locationTbl$countryCode[sites_locationTbl$timezone == "America/Puerto Rico"] <- "PR"

# ----- Replace stateCodes -----------------------------------------------------

# NOTE:  AQS 'State Code' values are a mess because they are naively derived
# NOTE:  from 9-digit AQS code positions and are incorrect for 12-digit AQS codes.

MazamaSpatialUtils::loadSpatialData("NaturalEarthAdm1")

sites_locationTbl$stateCode <-
  MazamaSpatialUtils::getStateCode(
    sites_locationTbl$longitude,
    sites_locationTbl$latitude,
    # NOTE:  EPA has monitors from US, Canada, Mexico, Puerto Rico, Virgin Islands and Guam
    countryCodes = c("US", "CA", "MX", "PR", "VI", "GU"),
    useBuffering = TRUE
)

```

## Review

It's now time to review our first pass table, `sites_locationTbl`:

```{r first_pass_review}
# What country codes to we have?
table(sites_locationTbl$countryCode)

# Are any locations duplicated?
any(duplicated(sites_locationTbl$locationID))
```

## Deal with duplicated sites

Unfortunately, we have duplicated locations in the `sites_locationTbl`. We can 
review these locations with the following code:

```{r review_duplicated_sites}
mask <- duplicated(sites_locationTbl$locationID)
duplicate_locationIDs <- sites_locationTbl$locationID[mask]

# # Review duplicates visually
# sites_locationTbl %>%
#   dplyr::filter(locationID %in% duplicate_locationIDs) %>%
#   View()

# Show the differentiating fields
sites_locationTbl %>%
  dplyr::filter(locationID %in% duplicate_locationIDs) %>%
  dplyr::select(airnow_status, airnow_AQSID, airnow_agencyName)
```

Most of these sites are `Inactive`. We choose to remove the 
duplicated sites by ordering based on status, which will place "Active" ahead of
"Inactive", and accepting whichever one comes first.

```{r remove_duplicated_sites}
sites_locationTbl <-
  sites_locationTbl %>% 
  dplyr::arrange(airnow_status) %>%
  dplyr:::distinct(locationID, .keep_all = TRUE)

# Are any locations duplicated?
any(duplicated(sites_locationTbl$locationID))
```

Now that we have truly unique locations, we can move to final steps.

# Final Steps

## Remove unwanted columns

We will remove only a few redundant columns.

```{r remove_non_spatial, results = "hold"}
# Remove unwanted columns
unwanted_columns <- c(
  # Renamed or redundant
  "airnow_FIPSStateCode",
  "airnow_GNISCountyCode"
)

sites_locationTbl <- 
  sites_locationTbl %>%
  dplyr::select(- dplyr::all_of(unwanted_columns))
```

## Finishing touches

After a quick review with `View(sites_locationTbl)`, a few more _data janitor_ 
tasks can be dealt with here:

```{r finishing_touches, results = "hold"}
# Rename AQSID
sites_locationTbl <-
  sites_locationTbl %>%
  dplyr::rename(
    AQSID = airnow_AQSID
  )

# Uniform casing of locationName and countyName
sites_locationTbl$locationName <- 
  stringr::str_to_title(sites_locationTbl$locationName)

sites_locationTbl$countyName <- 
  stringr::str_to_title(sites_locationTbl$countyName)

# Use NA where appropriate
sites_locationTbl <-
  sites_locationTbl %>%
  dplyr::mutate(locationName = dplyr::na_if(locationName, "N/A")) %>%
  dplyr::mutate(countyName = dplyr::na_if(countyName, "N/A"))

# Reset incorrect county names in foreign countries
sites_locationTbl$countyName[sites_locationTbl$countryCode == "CA"] <- NA
sites_locationTbl$countyName[sites_locationTbl$countryCode == "MX"] <- NA

dplyr::glimpse(sites_locationTbl)
```

## Visual spot check

We can visually spot check the result with leaflet. Clicking on a location will
display core metadata.

```{r final_result_leaflet}
table_leaflet(
  sites_locationTbl,
  extraVars = c("AQSID", "airnow_agencyName")
)
```

## Fix a few border locations

A couple of locations along border have incorrect state codes. We fix those
manuall here:

```{r manually_fix_border_sites}
sites_locationTbl$stateCode[sites_locationTbl$AQSID == "160211007"] <- "ID"
sites_locationTbl$stateCode[sites_locationTbl$AQSID == "000065701"] <- "ON"
sites_locationTbl$stateCode[sites_locationTbl$AQSID == "000041501"] <- "NB"
sites_locationTbl$stateCode[sites_locationTbl$AQSID == "480610006"] <- "TX"
sites_locationTbl$stateCode[sites_locationTbl$AQSID == "484790313"] <- "TX"
sites_locationTbl$stateCode[sites_locationTbl$AQSID == "481410055"] <- "TX"
sites_locationTbl$stateCode[sites_locationTbl$AQSID == "481410053"] <- "TX"
```

## Save our work

We will save our newly vetted "known locations" table in a dedicated directory.

```{r save}
setLocationDataDir("~/Data/Known_Locations")
table_save(sites_locationTbl, "airnow_PM2.5_sites")

table_csv <- table_export(sites_locationTbl)
readr::write_csv(sites_locationTbl, file = file.path(getLocationDataDir(), "airnow_PM2.5_sites.csv"))
```

# Compare with EPA AQS 88101

Let's compare this table with sites from EPA AQS:

```{r compare_88101_42101}
# Set "known locations" directory
MazamaLocationUtils::setLocationDataDir("~/Data/known_locations")

kl_airnow_PM2.5 <- MazamaLocationUtils::table_load("airnow_PM2.5_sites")
kl_88101 <- MazamaLocationUtils::table_load("AQS_88101_sites")

kl_airnow_PM2.5 %>% 
  MazamaLocationUtils::table_leaflet(
    opacity = 0.0,
    color = "black"
  ) %>% 
  MazamaLocationUtils::table_leafletAdd(
    kl_88101, 
    weight = 2,
    color = "black",
    fillOpacity = 0
  )
```

Compared with 88101 (PM2.5), 42101 (Ozone) monitors are more concentrated in
urban areas.
