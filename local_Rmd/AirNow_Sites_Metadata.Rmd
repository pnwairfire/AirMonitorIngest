---
title: AirNow Sites Metadata Review
author: 
- name: Jonathan Callahan
  email: jonathan.callahan@dri.edu
  affiliation: Desert Research Institute
date: October 21, 2021
output:
  html_document:
    toc: TRUE
    toc_depth: 3
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This review looks at the Sites Metadata document produced by AirNow and available
at https://files.airnowtech.org/airnow/today/monitoring_site_locations.dat.

A [Factsheet](https://www.airnowapi.org/docs/MonitoringSiteFactSheet.pdf) 
describes the file format.

This review is written as an Rmarkdown document and can be reproduced in its 
entirety by anyone comfortable using R and RStudio. For further analysis, users
should be familiar with "tidyverse" packages like **readr**, **stringr**, and 
**dplyr**.

The latest development versions of the following MazamaScience R packages are 
also required:

* [MazamaSpatialUtils](https://mazamascience.github.io/MazamaSpatialUtils/)
* [MazamaLocationUtils](https://mazamascience.github.io/MazamaLocationUtils/)

These can be installed with:

```
devtools::install_github("MazamaScience/MazamaSpatialUtils")
devtools::install_github("MazamaScience/MazamaLocationUtils")
```

# Summary Recommendations

For those in a hurry, the recommendations resulting from this review
are summarized below. These recommendations focus on a few simple fixes 
that would have the greatest impact.

1. Update the [Factsheet](https://www.airnowapi.org/docs/MonitoringSiteFactSheet.pdf)
with correct information about:
   - all fields including empty ones
   - text encoding
   - _caveat_ about elevations marked as `0.0`
2. Do not modify `monitoring_site_locations.dat`. (An unknown number of clients
already work with this format.)
3. For improvements, create a new `monitor_site_locations_2.dat`.
4. Extract correct "State Code" from 12-digit AQSIDs
5. Extract correct "County Code" from 12-digit AQSIDs
6. Repair "State Name" based on the correct "State Code"
7. Mark as missing any "State Name", "State Code" or "County Code" generated
from AQSIDs with characters.
 
Just fixing these few items will greatly improve the utility of the sites 
metadata file.

Correct spatial information for all records can be assigned based purely on 
longitudes and latitudes using **MazamaSpatialUtils** and **MazamaLocationUtils** 
but that is beyond the scope of this review.

# Reading in the Data

## Issue 1 -- Missing Fields in Format Description

The Factsheet description of the format does not match the example records in 
a way that is helpful to people writing data ingest code. the "corrected"
description below highlights the issue.

**Factsheet format description:**

AQSID|parameter name|site code|site name|status|
agency id|agency name|EPA region|latitude|longitude|
elevation|GMT offset|country code|MSA code|MSA name|
state code|state name|county code|county name

**Factsheet first example:**

060410001|O3|0001|San Rafael|Active|
CA2|San Francisco Bay Area AQMD|R9|37.972200|-122.518900|
0.900|- 8.00|US|||
41860| San Francisco-Oakland-Fremont, CA |06|CA|06041|
MARIN||

**Corrected format description:**

AQSID|parameter name|site code|site name|status|
agency id|agency name|EPA region|latitude|longitude|
elevation|GMT offset|country code|**empty1**|**empty2**|
MSA code|MSA name|state code|state name|county code|
county name|**empty3**|**empty4**


**The _empty#_ fields need to be defined in order to automate data ingest.**

With these fields defined, the file can be read in:

```{r parse_1, message = FALSE, results = "hold"}
library(dplyr)

col_names <- c(
  "AQSID", "parameterName", "siteCode", "siteName", "status",
  "agencyID", "agencyName", "EPARegion", "latitude", "longitude",
  "elevation", "GMTOffsetHours", "countryCode", "empty1", "empty2",
  "MSACode", "MSAName", "stateCode", "stateName", "countyCode",
  "countyName", "empty3", "empty4"
)

col_types <- paste0("ccccc", "cccdd", "ddccc", "ccccc", "ccc")

url <- "https://files.airnowtech.org/airnow/today/monitoring_site_locations.dat"

# Read in .dat file as a tibble
full_sites <- 
  readr::read_delim(
    url,
    delim = "|",
    col_names = col_names,
    col_types = col_types
  )

# Take a quick look at the table
dplyr::glimpse(full_sites)
```

## Issue 2 -- CP437 encoding

A careful peruse through the data shows some encoding issues:

```{r encoding, message = FALSE, results = "hold"}
full_sites %>%
  dplyr::filter(AQSID == "800150581", parameterName == "PM2.5") %>%
  dplyr::pull(siteName)
```

Some experimentation reveals that this file is encoded with 
[CP437](https://en.wikipedia.org/wiki/Code_page_437), typically associated with
older IBM PCs. The `readr::read_delim()` default encoding is "UTF-8".

To correct this, we can download and parse the file again, this time using the
"CP437" encoding:

```{r parse_2, message = FALSE, results = "hold"}
# Read in .dat file as a tibble
full_sites <- 
  readr::read_delim(
    url,
    delim = "|",
    col_names = col_names,
    col_types = col_types,
    locale = readr::locale(encoding = "CP437")
  )

# Did we correct the special characters?
full_sites %>%
  dplyr::filter(AQSID == "800150581", parameterName == "PM2.5") %>%
  dplyr::pull(siteName)
```

Excellent!

# Validating Fields

## Longitude and latitude

We will limit our focus to PM2.5 in North America and will generate a quick map
for visual inspection. This is the fastest way to validate location information.

```{r map_1, results = "hold"}
# Reduce the size of the sites
sites <-
  full_sites %>%
  # drop the "empty" fields
  dplyr::select(-dplyr::starts_with("empty")) %>%
  # filter based on various fields
  dplyr::filter(parameterName == "PM2.5") %>%
  dplyr::filter(GMTOffsetHours < 0) %>%
  dplyr::filter(latitude > 15.0)

# Display in an interactive map
MazamaLocationUtils::table_leaflet(
  sites,
  extraVars = c("elevation", "status")
)
```

When you click on a site, additional site metadata will be
displayed.

**_NOTE:_ Many of the elevations are incorrectly set to 0.0.**

Other than the elevation data, things look reasonable. Now lets begin validating 
the contents of other fields.

```{r names}
names(sites)
```

At this point, an interactive session using `View()` is very helpful for 
a detailed examination of the dataset:

```
View(sites)
```

## Status and countryCode

These fields have only a few possible values. We can inspect them directly:

```{r table_1}
table(sites$status) %>% sort(decreasing = TRUE)
table(sites$countryCode) %>% sort(decreasing = TRUE)
```

These look good.

## EPA Region

```{r table_2}
table(sites$EPARegion) %>% sort(decreasing = TRUE)
```

The `EPARegion` field has values of `R1` - `R10` as well as `CA`, `DSMX`, `MX`
and `USEPA`. It makes sense that sites in Canada and Mexico do not fit in an 
EPA Region, but what about `USEPA`?

```{r EPARegion_USEPA}
sites %>% 
  dplyr::filter(EPARegion == "USEPA") %>% 
  # Sample to reduce the size
  dplyr::slice_sample(n = 20) %>%
  dplyr::select(AQSID, agencyName, countryCode, stateCode, countyCode)
```

This is a mixed bag but, except for "State of Alaska DEC", sites appear to be 
associated with federal rather than state agencies.

## Issue 3 -- Non-standard AQSIDs generate bogus state and county codes

Some of the sites with `EPARegion == "USEPA"` have AQSIDs that do not match the
description in the Factsheet: "Nine-digit EPA AQS identifier". 

The canonical 
9-digit AQSID consists of a 2-digit FIPS state code followed by a 3-digit FIPS 
county-within-state code followed by a 4-digit site identifier within that county.

It appears that the `stateCode` and `countyCode` fields are being generated from
the first 2 and 5 characters of AQSID _even for non-standard AQSIDs_. This generates
bogus results for `stateCode` and `countyCode`.

Let's check standard and non-standard AQSIDs:

```{r AQSID_standard}
# Standard AQSIDs
sites %>%
  # Filter for 9-digit AQSIDs
  dplyr::filter(stringr::str_detect(AQSID, "^[:digit:]{9}$")) %>%
  # Sample to reduce the size
  dplyr::slice_sample(n = 20) %>%
  dplyr::select(AQSID, countryCode, stateCode, countyCode)
```

For standard AQSIDS, both `stateCode` and `countyCode` look as expected.

```{r AQSID_non_standard}
# Non-standard AQSIDs
sites %>%
  # Filter for NOT 9-digit AQSIDs
  dplyr::filter(stringr::str_detect(AQSID, "^[:digit:]{9}$", negate = TRUE)) %>%
  # Sample to reduce the size
  dplyr::slice_sample(n = 20) %>%
  dplyr::select(AQSID, stateName, stateCode, countyCode)
```

**For non-standard AQSIDs, `stateCode` and `countyCode` are incorrect.**

The `stateName` field has many correct USPS state codes but also includes 
`CC`, `MM`, `MX` and `TT` which are not valid USPS state codes.

See [ANSI Codes for States](https://www.census.gov/library/reference/code-lists/ansi/ansi-codes-for-states.html).

## Interrogate non-standard AQSIDs

The non-standard AQSIDs all appear to have an extra 3 digits prepended which
matches the "full AQSID" found in data requested from the AirNow API data webservice.

```{r AQSID_length}
# How long are strings in the AQSID field?
table(stringr::str_length(sites$AQSID))
```

The good news is that AQSID has only one non-standard length.

What 3-character prefixes are being used?

```{r AQSID_prefixes}
# What prefixes are found in non-standard AQSIDs?
sites$AQSID %>% 
  stringr::str_subset("^[:alnum:]{12}$") %>% 
  stringr::str_sub(1,3) %>% 
  table() %>% 
  sort(decreasing = TRUE)
```

The "840" prefix is by far the most common.

What about standard-length AQSIDs with characters? It appears that the first
two characters have a different meaning than "state code":

```{r character_AQSID_state_code}
# "state code" for AQSIDs with characters?
sites$AQSID %>% 
  stringr::str_subset("^[:alnum:]{9}$") %>% 
  stringr::str_subset("[:alpha:]") %>% 
  stringr::str_sub(1,2) %>% 
  table() %>% 
  sort(decreasing = TRUE)
```

We can review these records with `View()`:

```
sites %>%
  dplyr::filter(stringr::str_detect(AQSID, "^[:alnum:]{9}$")) %>% 
  dplyr::filter(stringr::str_detect(AQSID, "[:alpha:]")) %>% 
  View()
```

It appears that:

* `MM` is used for _Mobile_ monitors
* `80` is used for monitors in _Mexico_
* `CC` is used for monitors in _Canada_
* `TT` is used for monitors on _Tribal_ lands
* `15` is a "one off" exception in Hawaii

# Correcting State and County Codes and Names

It appears that we can correct many of the bogus state and county codes that
exist in the dataset by following this simple protocol:

1. reset all state and county names and codes to "missing"
2. For 9-character, all-digit AQSIDs
 - state code = digits 1:2
 - county code = digits 1:5
3. For 12-character, all-digit AQSIDs
 - state code = digits 4:5
 - county code = digits 4:8
4. Replace state name based on the corrected state code
 
Let's try it out:

```{r fix_state_county_codes}
# 1) Replace values with the AirNow code for "missing"
sites$stateCode <- "N/A"
sites$stateName <- "N/A"
sites$countyCode <- "N/A"

# 2) 9-character, all-digit mask
mask <- stringr::str_detect(sites$AQSID, "^[:digit:]{9}$")
sites$stateCode[mask] <- stringr::str_sub(sites$AQSID[mask], 1, 2)
sites$countyCode[mask] <- stringr::str_sub(sites$AQSID[mask], 1, 5)

# 3) 12-character, all-digit mask
mask <- stringr::str_detect(sites$AQSID, "^[:digit:]{12}$")
sites$stateCode[mask] <- stringr::str_sub(sites$AQSID[mask], 4, 5)
sites$countyCode[mask] <- stringr::str_sub(sites$AQSID[mask], 4, 8)

# 4) Replace state name
sites$stateName <- MazamaSpatialUtils::US_stateFIPSToCode(sites$stateCode)

# Check our results interactively
MazamaLocationUtils::table_leaflet(
  sites,
  extraVars = c("countryCode", "stateName", "stateCode", "countyCode")
)
```

Things look much better now.

----

This ends the quick review of the AirNow sites metadata file.

I hope this review points out a few improvements that can be quickly implemented 
for an improved sites metadata file.

Hopefully, it also demonstrates how data validation like this can be done in
an _open_, _transparent_ and _reproducible_ manner.

_Best hopes for improved site metadata!_
